# Databricks notebook source
dbutils.widgets.text("vs_endpoint_name", "", "Vector Search Endpoint Name")
dbutils.widgets.text("sitemap_urls", "", "URLs separated by comma of sitemap.xml")
dbutils.widgets.text(
    "accepted_domains", "", "Domain part of urls to process, separated by comma"
)
dbutils.widgets.text(
    "catalog", "", "Catalog name where data and vector search are stored"
)
dbutils.widgets.text(
    "schema_name", "", "Schema name where data and vector search are stored"
)

# COMMAND ----------

# MAGIC %pip install mlflow==2.10.1 lxml transformers==4.30.2 langchain==0.1.5 databricks-vectorsearch==0.22 beautifulsoup4
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %run ../_resources/00-init $reset_all_data=false $vs_endpoint_name=$vs_endpoint_name $sitemap_urls=$sitemap_urls $accepted_domains=$accepted_domains $catalog=$catalog $schema_name=$schema_name

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC ### Creating our Vector Search Index with Managed Embeddings and BGE
# MAGIC
# MAGIC <img src="https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-3.png?raw=true" style="float: right; width: 600px; margin-left: 10px">
# MAGIC
# MAGIC With Managed embeddings, Databricks will automatically compte the embeddings for us. This is the easier mode to get started with Databricks.
# MAGIC
# MAGIC A vector search index uses a **Vector search endpoint** to serve the embeddings (you can think about it as your Vector Search API endpoint).
# MAGIC
# MAGIC Multiple Indexes can use the same endpoint. 
# MAGIC
# MAGIC Let's start by creating one.

# COMMAND ----------

# MAGIC %sql
# MAGIC --Note that we need to enable Change Data Feed on the table to create the index
# MAGIC CREATE TABLE IF NOT EXISTS documentation (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   url STRING,
# MAGIC   content STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true);

# COMMAND ----------

if not table_exists("documentation") or spark.table("documentation").isEmpty():
    (
        spark.table("web_documentation")
        .unionAll(spark.table("pdf_documentation"))
        .write.mode("overwrite")
        .saveAsTable("documentation")
    )

display(spark.table("documentation"))

# COMMAND ----------

from databricks.vector_search.client import VectorSearchClient

vsc = VectorSearchClient()

if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):
    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type="STANDARD")

wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)
print(f"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.")

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC
# MAGIC <img src="https://github.com/databricks-demos/dbdemos-resources/blob/main/images/index_creation.gif?raw=true" width="600px" style="float: right">
# MAGIC
# MAGIC You can view your endpoint on the [Vector Search Endpoints UI](#/setting/clusters/vector-search). Click on the endpoint name to see all indexes that are served by the endpoint.
# MAGIC
# MAGIC
# MAGIC ### Creating the Vector Search Index
# MAGIC
# MAGIC All we now have to do is to as Databricks to create the index. 
# MAGIC
# MAGIC Because it's a managed embedding index, we just need to specify the text column and our embedding foundation model (`BGE`).  Databricks will compute the embeddings for us automatically.
# MAGIC
# MAGIC This can be done using the API, or in a few clicks within the Unity Catalog Explorer menu.
# MAGIC

# COMMAND ----------

from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c

# The table we'd like to index
source_table_fullname = f"{catalog}.{db}.documentation"
# Where we want to store our index
vs_index_fullname = f"{catalog}.{db}.documentation_vs_index"

if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):
    print(
        f"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}..."
    )
    vsc.create_delta_sync_index(
        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,
        index_name=vs_index_fullname,
        source_table_name=source_table_fullname,
        pipeline_type="TRIGGERED",
        primary_key="id",
        embedding_source_column="content",  # The column containing our text
        embedding_model_endpoint_name="databricks-bge-large-en",  # The embedding endpoint used to create the embeddings
    )
    # Let's wait for the index to be ready and all our embeddings to be created and indexed
    wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)
else:
    # Trigger a sync to update our vs content with the new data saved in the table
    wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)
    vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()

print(f"index {vs_index_fullname} on table {source_table_fullname} is ready")

# COMMAND ----------

import mlflow.deployments

deploy_client = mlflow.deployments.get_deploy_client("databricks")

question = "How much does it cost?"

results = vsc.get_index(
    VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname
).similarity_search(query_text=question, columns=["url", "content"], num_results=1)
docs = results.get("result", {}).get("data_array", [])
docs
